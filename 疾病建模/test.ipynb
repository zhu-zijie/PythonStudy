{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 数据导入和预处理",
   "id": "5111fdb6c1f26456"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, \\\n",
    "    precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import warnings\n",
    "\n",
    "# 过滤sklearn的数值计算警告\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='numpy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### heart建模",
   "id": "c42a9d61d1c6a8d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 读取已清理的数据\n",
    "df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "\n",
    "# 对分类变量进行One-Hot编码\n",
    "df_heart = pd.get_dummies(df_heart, columns=[\n",
    "    'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "], drop_first=True)\n",
    "\n",
    "# 将目标变量转换为数值格式\n",
    "le = LabelEncoder()\n",
    "df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "print(f\"目标变量映射: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = df_heart.drop('HeartDisease', axis=1)\n",
    "y = df_heart['HeartDisease']\n",
    "\n",
    "# 确保数据类型正确\n",
    "X = X.astype('float64')\n",
    "\n",
    "# 数值稳定性检查\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "if X.isnull().any().any():\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 使用RobustScaler进行特征缩放\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将缩放后的数据转换回DataFrame格式\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)"
   ],
   "id": "341949ea6b93db97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 构建逻辑回归模型\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver='lbfgs',\n",
    "    C=1.0,\n",
    "    max_iter=2000\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 模型预测\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 模型评估\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Heart Disease Prediction Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Heart Disease Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"heart_roc_curve.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 特征重要性分析\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)"
   ],
   "id": "da46c47bb3799c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 随机森林模型\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}, ROC AUC: {rf_roc_auc:.4f}\")\n",
    "\n",
    "# 特征重要性\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_importance.head(10))"
   ],
   "id": "ef3575ebb58e04a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVM模型\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_pred_proba)\n",
    "\n",
    "print(f\"SVM - Accuracy: {svm_accuracy:.4f}, ROC AUC: {svm_roc_auc:.4f}\")"
   ],
   "id": "2858317407707c88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 梯度提升模型\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "gb_pred = gb_model.predict(X_test_scaled)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "print(f\"Gradient Boosting - Accuracy: {gb_accuracy:.4f}, ROC AUC: {gb_roc_auc:.4f}\")\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Feature Importance (Top 10):\")\n",
    "print(gb_importance.head(10))"
   ],
   "id": "831b840aa3baba3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KNN模型\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_roc_auc = roc_auc_score(y_test, knn_pred_proba)\n",
    "\n",
    "print(f\"KNN - Accuracy: {knn_accuracy:.4f}, ROC AUC: {knn_roc_auc:.4f}\")"
   ],
   "id": "109279c3fd56539a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LightGBM模型\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    verbose=-1  # 减少输出信息\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_scaled)\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "lgb_roc_auc = roc_auc_score(y_test, lgb_pred_proba)\n",
    "\n",
    "print(f\"LightGBM - Accuracy: {lgb_accuracy:.4f}, ROC AUC: {lgb_roc_auc:.4f}\")\n",
    "\n",
    "# LightGBM特征重要性\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': lgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"LightGBM Feature Importance (Top 10):\")\n",
    "print(lgb_importance.head(10))"
   ],
   "id": "75984d68db460976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# XGBoost模型\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0  # 减少输出信息\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "print(f\"XGBoost - Accuracy: {xgb_accuracy:.4f}, ROC AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# XGBoost特征重要性\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"XGBoost Feature Importance (Top 10):\")\n",
    "print(xgb_importance.head(10))"
   ],
   "id": "5d44e527fdfccce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 模型性能比较\n",
    "model_results_extended = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'KNN', 'LightGBM', 'XGBoost'],\n",
    "    'Accuracy': [accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, lgb_accuracy, xgb_accuracy],\n",
    "    'ROC_AUC': [roc_auc, rf_roc_auc, svm_roc_auc, gb_roc_auc, knn_roc_auc, lgb_roc_auc, xgb_roc_auc]\n",
    "}).sort_values(by='ROC_AUC', ascending=False)\n",
    "\n",
    "print(\"Extended Model Performance Comparison:\")\n",
    "print(model_results_extended)\n",
    "\n",
    "# 可视化扩展比较\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(model_results_extended['Model'], model_results_extended['Accuracy'])\n",
    "plt.title('Model Accuracy Comparison (Extended)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(model_results_extended['Model'], model_results_extended['ROC_AUC'])\n",
    "plt.title('Model ROC AUC Comparison (Extended)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('ROC AUC')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"extended_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best performing model: {model_results_extended.iloc[0]['Model']}\")"
   ],
   "id": "d0166aa8440f422c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 交叉验证评估\n",
    "models_extended = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, solver='lbfgs', max_iter=2000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, verbosity=0)\n",
    "}\n",
    "\n",
    "print(\"=== 交叉验证结果 ===\")\n",
    "cv_results_extended = {}\n",
    "for name, model in models_extended.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    cv_results_extended[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} - CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ],
   "id": "947fa6af76f2dd26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 特征重要性综合分析\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 显示树模型的特征重要性\n",
    "tree_models = [\n",
    "    ('Random Forest', rf_importance),\n",
    "    ('Gradient Boosting', gb_importance),\n",
    "    ('LightGBM', lgb_importance),\n",
    "    ('XGBoost', xgb_importance)\n",
    "]\n",
    "\n",
    "for i, (name, importance_df) in enumerate(tree_models):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.title(f'{name} Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tree_models_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "ce4f0f374dbaf27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### stroke建模",
   "id": "9d1b2888b0b2e9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 读取已清理的数据\n",
    "df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "\n",
    "# 特征工程和编码\n",
    "df_stroke = pd.get_dummies(df_stroke,\n",
    "                           columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status',\n",
    "                                    'hypertension', 'heart_disease'], drop_first=True)\n",
    "\n",
    "# 编码目标变量\n",
    "le = LabelEncoder()\n",
    "df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "print(f\"目标变量映射: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "y = df_stroke['stroke_encoded']\n",
    "\n",
    "# 添加：数据不平衡分析\n",
    "print(\"\\n=== 数据不平衡分析 ===\")\n",
    "print(f\"特征数量: {X.shape[1]}\")\n",
    "print(f\"目标变量分布:\\n{y.value_counts()}\")\n",
    "print(f\"正例比例: {y.mean():.4f}\")\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 特征缩放\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "3839b9f16e2aeb65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 使用平衡权重的逻辑回归模型\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    solver='liblinear',\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 模型预测\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 找到最佳分类阈值\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "\n",
    "print(f\"\\n=== 阈值优化 ===\")\n",
    "print(f\"默认阈值(0.5)预测正例数: {(y_pred == 1).sum()}\")\n",
    "print(f\"最佳阈值: {best_threshold:.4f}\")\n",
    "\n",
    "# 使用最佳阈值重新预测\n",
    "y_pred_optimal = (y_pred_proba >= best_threshold).astype(int)\n",
    "print(f\"最佳阈值预测正例数: {(y_pred_optimal == 1).sum()}\")\n",
    "\n",
    "# 模型评估\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# 默认阈值评估\n",
    "class_report = classification_report(\n",
    "    y_test, y_pred,\n",
    "    zero_division=0,\n",
    "    target_names=['No Stroke', 'Stroke']\n",
    ")\n",
    "\n",
    "# 最佳阈值评估\n",
    "accuracy_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "class_report_optimal = classification_report(\n",
    "    y_test, y_pred_optimal,\n",
    "    zero_division=0,\n",
    "    target_names=['No Stroke', 'Stroke']\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Stroke Prediction Model Evaluation ---\")\n",
    "print(f\"Accuracy (默认阈值): {accuracy:.4f}\")\n",
    "print(f\"Accuracy (最佳阈值): {accuracy_optimal:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(\"\\nClassification Report (默认阈值):\\n\", class_report)\n",
    "print(f\"\\nClassification Report (最佳阈值 {best_threshold:.3f}):\\n\", class_report_optimal)\n",
    "\n",
    "# 绘制综合分析图\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# ROC曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve')\n",
    "plt.axvline(x=recall[best_threshold_idx], color='red', linestyle='--',\n",
    "            label=f'Best threshold: {best_threshold:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 预测概率分布\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, label='No Stroke', color='blue')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, label='Stroke', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Default threshold')\n",
    "plt.axvline(x=best_threshold, color='green', linestyle='--', label='Optimal threshold')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Probability Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stroke_comprehensive_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 特征重要性分析\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Logistic Regression Coefficients):\\n\", feature_importance.head(10))\n",
    "\n",
    "# 逻辑回归特征\n",
    "lr_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': np.abs(model.coef_[0])  # 使用系数绝对值作为重要性\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 可视化逻辑回归特征重要性\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = lr_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.title('Stroke Logistic Regression Feature Importance')\n",
    "plt.xlabel('Importance Score (|Coefficient|)')\n",
    "plt.gca().invert_yaxis()  # 让最重要的特征显示在顶部\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stroke_logistic_regression_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "c1969bd00a244330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 随机森林模型\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}, ROC AUC: {rf_roc_auc:.4f}\")\n",
    "\n",
    "# 特征重要性\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_importance.head(10))"
   ],
   "id": "2ab9675d99404981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVM模型\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_pred_proba)\n",
    "\n",
    "print(f\"SVM - Accuracy: {svm_accuracy:.4f}, ROC AUC: {svm_roc_auc:.4f}\")"
   ],
   "id": "88f52f2cc6797c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 梯度提升模型\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "gb_pred = gb_model.predict(X_test_scaled)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "print(f\"Gradient Boosting - Accuracy: {gb_accuracy:.4f}, ROC AUC: {gb_roc_auc:.4f}\")\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Feature Importance (Top 10):\")\n",
    "print(gb_importance.head(10))"
   ],
   "id": "295fd8abf3697fcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KNN模型\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_roc_auc = roc_auc_score(y_test, knn_pred_proba)\n",
    "\n",
    "print(f\"KNN - Accuracy: {knn_accuracy:.4f}, ROC AUC: {knn_roc_auc:.4f}\")"
   ],
   "id": "81349045d720fdce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LightGBM模型\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    verbose=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# 使用DataFrame格式训练和预测\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "lgb_model.fit(X_train_df, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_df)\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test_df)[:, 1]\n",
    "\n",
    "lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "lgb_roc_auc = roc_auc_score(y_test, lgb_pred_proba)\n",
    "\n",
    "print(f\"LightGBM - Accuracy: {lgb_accuracy:.4f}, ROC AUC: {lgb_roc_auc:.4f}\")"
   ],
   "id": "8d73508940ecb782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# XGBoost模型\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0,\n",
    "    scale_pos_weight=scale_pos_weight  # 处理不平衡数据\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "print(f\"XGBoost - Accuracy: {xgb_accuracy:.4f}, ROC AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "# XGBoost特征重要性\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"XGBoost Feature Importance (Top 10):\")\n",
    "print(xgb_importance.head(10))"
   ],
   "id": "3abeba5dba1f01bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 模型性能比较\n",
    "model_results_stroke = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'KNN', 'LightGBM', 'XGBoost'],\n",
    "    'Accuracy': [accuracy_optimal, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, lgb_accuracy, xgb_accuracy],\n",
    "    'ROC_AUC': [roc_auc, rf_roc_auc, svm_roc_auc, gb_roc_auc, knn_roc_auc, lgb_roc_auc, xgb_roc_auc]\n",
    "}).sort_values(by='ROC_AUC', ascending=False)\n",
    "\n",
    "print(\"\\nStroke Model Performance Comparison:\")\n",
    "print(model_results_stroke)\n",
    "\n",
    "# 可视化比较\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(model_results_stroke['Model'], model_results_stroke['Accuracy'])\n",
    "plt.title('Stroke Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(model_results_stroke['Model'], model_results_stroke['ROC_AUC'])\n",
    "plt.title('Stroke Model ROC AUC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stroke_extended_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best performing model: {model_results_stroke.iloc[0]['Model']}\")"
   ],
   "id": "4747b6cb2937c645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 交叉验证评估\n",
    "models_stroke = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced',\n",
    "                                              max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1, class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, verbosity=0, scale_pos_weight=scale_pos_weight)\n",
    "}\n",
    "\n",
    "print(\"\\n=== Stroke 交叉验证结果 ===\")\n",
    "cv_results_stroke = {}\n",
    "for name, model in models_stroke.items():\n",
    "    cv_scores = cross_val_score(model, X_train_df, y_train, cv=5, scoring='roc_auc')\n",
    "    cv_results_stroke[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} - CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ],
   "id": "fcdd10d0b33e235b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 特征重要性综合分析\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 显示树模型的特征重要性\n",
    "tree_models_stroke = [\n",
    "    ('Random Forest', rf_importance),\n",
    "    ('Gradient Boosting', gb_importance),\n",
    "    ('LightGBM', lgb_importance),\n",
    "    ('XGBoost', xgb_importance)\n",
    "]\n",
    "\n",
    "for i, (name, importance_df) in enumerate(tree_models_stroke):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.title(f'Stroke {name} Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stroke_tree_models_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 针对stroke数据的阈值优化分析\n",
    "print(\"\\n=== 最佳模型阈值优化 ===\")\n",
    "best_model_name = model_results_stroke.iloc[0]['Model']\n",
    "print(f\"选择最佳模型: {best_model_name}\")\n",
    "\n",
    "# 获取最佳模型的预测概率\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_pred_proba = rf_pred_proba\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_pred_proba = xgb_pred_proba\n",
    "elif best_model_name == 'LightGBM':\n",
    "    best_pred_proba = lgb_pred_proba\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_pred_proba = gb_pred_proba\n",
    "elif best_model_name == 'SVM':\n",
    "    best_pred_proba = svm_pred_proba\n",
    "elif best_model_name == 'KNN':\n",
    "    best_pred_proba = knn_pred_proba\n",
    "else:\n",
    "    best_pred_proba = y_pred_proba\n",
    "\n",
    "# 为最佳模型找最佳阈值\n",
    "precision_best, recall_best, thresholds_best = precision_recall_curve(y_test, best_pred_proba)\n",
    "f1_scores_best = 2 * (precision_best * recall_best) / (precision_best + recall_best)\n",
    "best_threshold_idx_best = np.argmax(f1_scores_best)\n",
    "best_threshold_best = thresholds_best[best_threshold_idx_best]\n",
    "\n",
    "print(f\"最佳模型的最佳阈值: {best_threshold_best:.4f}\")\n",
    "\n",
    "# 使用最佳阈值重新评估\n",
    "y_pred_best_optimal = (best_pred_proba >= best_threshold_best).astype(int)\n",
    "accuracy_best_optimal = accuracy_score(y_test, y_pred_best_optimal)\n",
    "class_report_best_optimal = classification_report(\n",
    "    y_test, y_pred_best_optimal,\n",
    "    zero_division=0,\n",
    "    target_names=['No Stroke', 'Stroke']\n",
    ")\n",
    "\n",
    "print(f\"最佳模型 + 最佳阈值准确率: {accuracy_best_optimal:.4f}\")\n",
    "print(f\"\\n最佳模型分类报告:\\n{class_report_best_optimal}\")"
   ],
   "id": "cea6c89b76ef00da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### cirrhosis建模",
   "id": "54696ec2e67788dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "\n",
    "# 特征工程和编码\n",
    "# 将目标变量Status映射为数值：D, C, CL\n",
    "status_mapping = {\n",
    "    'D': 1,  # Deceased\n",
    "    'C': 0,  # Censor\n",
    "    'CL': 0  # Censor (transplant)\n",
    "}\n",
    "df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "\n",
    "# 对分类变量进行One-Hot编码\n",
    "df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "    'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "], drop_first=True)\n",
    "\n",
    "# 删除原始Status列和ID列\n",
    "df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "y = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 特征缩放\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "fbe4dff00d97bdeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 构建逻辑回归模型\n",
    "model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 模型预测\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 模型评估\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Cirrhosis Prediction Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve for Cirrhosis Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"cirrhosis_roc_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "# 特征重要性分析 (对于逻辑回归，通过系数大小判断)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Logistic Regression Coefficients):\\n\", feature_importance)"
   ],
   "id": "3ad9fab8042ab57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 随机森林模型\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.4f}, ROC AUC: {rf_roc_auc:.4f}\")\n",
    "\n",
    "# 特征重要性\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Random Forest Feature Importance (Top 10):\")\n",
    "print(rf_importance.head(10))"
   ],
   "id": "b95ad4d8a15bc31d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 随机森林特征重要性单独可视化 - 改进版\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 创建更详细的可视化\n",
    "top_features = rf_importance.head(15)  # 显示更多特征\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.title('Cirrhosis Random Forest Feature Importance\\n(Based on Single Test Set)', fontsize=16, pad=20)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (bar, importance) in enumerate(zip(bars, top_features['Importance'])):\n",
    "    plt.text(importance + 0.001, bar.get_y() + bar.get_height() / 2,\n",
    "             f'{importance:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cirrhosis_random_forest_detailed_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 打印详细统计信息\n",
    "print(\"=== 随机森林特征重要性分析 ===\")\n",
    "print(\"前10个最重要特征:\")\n",
    "print(rf_importance.head(10).to_string(index=False))\n",
    "print(f\"\\n总特征数: {len(rf_importance)}\")\n",
    "print(f\"前5个特征重要性占比: {rf_importance.head(5)['Importance'].sum():.3f}\")\n",
    "print(f\"前10个特征重要性占比: {rf_importance.head(10)['Importance'].sum():.3f}\")"
   ],
   "id": "de487ffa2a6ec635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 随机森林特征重要性单独可视化\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = rf_importance.head(10)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.title('Cirrhosis Random Forest Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cirrhosis_random_forest_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 打印详细的特征重要性信息\n",
    "print(\"Random Forest Feature Importance (All Features):\")\n",
    "print(rf_importance)"
   ],
   "id": "fd9c38ebc4bb5649",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVM模型\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_pred_proba = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_pred_proba)\n",
    "\n",
    "print(f\"SVM - Accuracy: {svm_accuracy:.4f}, ROC AUC: {svm_roc_auc:.4f}\")"
   ],
   "id": "543c8b9274e007a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 梯度提升模型\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "gb_pred = gb_model.predict(X_test_scaled)\n",
    "gb_pred_proba = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "gb_roc_auc = roc_auc_score(y_test, gb_pred_proba)\n",
    "\n",
    "print(f\"Gradient Boosting - Accuracy: {gb_accuracy:.4f}, ROC AUC: {gb_roc_auc:.4f}\")\n",
    "\n",
    "gb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Gradient Boosting Feature Importance (Top 10):\")\n",
    "print(gb_importance.head(10))"
   ],
   "id": "fd95b43f2763d39f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# KNN模型\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "knn_pred_proba = knn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_roc_auc = roc_auc_score(y_test, knn_pred_proba)\n",
    "\n",
    "print(f\"KNN - Accuracy: {knn_accuracy:.4f}, ROC AUC: {knn_roc_auc:.4f}\")"
   ],
   "id": "9d682a3992fd7c12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LightGBM模型\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    verbose=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# 使用DataFrame格式训练和预测\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "lgb_model.fit(X_train_df, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_df)\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test_df)[:, 1]\n",
    "\n",
    "lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "lgb_roc_auc = roc_auc_score(y_test, lgb_pred_proba)\n",
    "\n",
    "print(f\"LightGBM - Accuracy: {lgb_accuracy:.4f}, ROC AUC: {lgb_roc_auc:.4f}\")\n",
    "\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': lgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"LightGBM Feature Importance (Top 10):\")\n",
    "print(lgb_importance.head(10))"
   ],
   "id": "833281cf06ae40b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# XGBoost模型\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_roc_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "print(f\"XGBoost - Accuracy: {xgb_accuracy:.4f}, ROC AUC: {xgb_roc_auc:.4f}\")\n",
    "\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"XGBoost Feature Importance (Top 10):\")\n",
    "print(xgb_importance.head(10))"
   ],
   "id": "971132e7b07b4c5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 模型性能比较\n",
    "model_results_cirrhosis = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'KNN', 'LightGBM', 'XGBoost'],\n",
    "    'Accuracy': [accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, lgb_accuracy, xgb_accuracy],\n",
    "    'ROC_AUC': [roc_auc, rf_roc_auc, svm_roc_auc, gb_roc_auc, knn_roc_auc, lgb_roc_auc, xgb_roc_auc]\n",
    "}).sort_values(by='ROC_AUC', ascending=False)\n",
    "\n",
    "print(\"\\nCirrhosis Model Performance Comparison:\")\n",
    "print(model_results_cirrhosis)\n",
    "\n",
    "# 可视化比较\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(model_results_cirrhosis['Model'], model_results_cirrhosis['Accuracy'])\n",
    "plt.title('Cirrhosis Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(model_results_cirrhosis['Model'], model_results_cirrhosis['ROC_AUC'])\n",
    "plt.title('Cirrhosis Model ROC AUC Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('ROC AUC')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cirrhosis_extended_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best performing model: {model_results_cirrhosis.iloc[0]['Model']}\")"
   ],
   "id": "4be73a4e10c5f73c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 交叉验证评估\n",
    "models_cirrhosis = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced',\n",
    "                                              max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'SVM': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=42, verbose=-1, class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42, verbosity=0, scale_pos_weight=scale_pos_weight)\n",
    "}\n",
    "\n",
    "print(\"\\n=== Cirrhosis 交叉验证结果 ===\")\n",
    "cv_results_cirrhosis = {}\n",
    "for name, model in models_cirrhosis.items():\n",
    "    cv_scores = cross_val_score(model, X_train_df, y_train, cv=5, scoring='roc_auc')\n",
    "    cv_results_cirrhosis[name] = {\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} - CV ROC AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ],
   "id": "b7cff9040e8fc1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 特征重要性综合分析\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 显示树模型的特征重要性\n",
    "tree_models_cirrhosis = [\n",
    "    ('Random Forest', rf_importance),\n",
    "    ('Gradient Boosting', gb_importance),\n",
    "    ('LightGBM', lgb_importance),\n",
    "    ('XGBoost', xgb_importance)\n",
    "]\n",
    "\n",
    "for i, (name, importance_df) in enumerate(tree_models_cirrhosis):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.title(f'Cirrhosis {name} Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cirrhosis_tree_models_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "67c30765447ed60b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模型准确性检验",
   "id": "7a86466deff422d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 综合模型准确性检验",
   "id": "4798a938d07ea508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def comprehensive_model_evaluation():\n",
    "    \"\"\"全面的模型准确性检验\"\"\"\n",
    "\n",
    "    # 准备最佳模型和对应的测试数据\n",
    "    evaluation_data = {\n",
    "        'Heart Disease': {\n",
    "            'model': model,  # 心脏病逻辑回归模型\n",
    "            'X_test': X_test_scaled,\n",
    "            'y_test': y_test,\n",
    "            'X_train': X_train_scaled,\n",
    "            'y_train': y_train\n",
    "        },\n",
    "        'Stroke': {\n",
    "            'model': model,  # 中风逻辑回归模型\n",
    "            'X_test': X_test_scaled,\n",
    "            'y_test': y_test,\n",
    "            'X_train': X_train_scaled,\n",
    "            'y_train': y_train\n",
    "        },\n",
    "        'Cirrhosis': {\n",
    "            'model': rf_model,  # 肝硬化随机森林模型\n",
    "            'X_test': X_test_scaled,\n",
    "            'y_test': y_test,\n",
    "            'X_train': X_train_scaled,\n",
    "            'y_train': y_train\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for disease, data in evaluation_data.items():\n",
    "        model_obj = data['model']\n",
    "        X_test_data = data['X_test']\n",
    "        y_test_data = data['y_test']\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model_obj.predict(X_test_data)\n",
    "        y_pred_proba = model_obj.predict_proba(X_test_data)[:, 1]\n",
    "\n",
    "        # 基础指标\n",
    "        accuracy = accuracy_score(y_test_data, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_test_data, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test_data, y_pred_proba)\n",
    "\n",
    "        # 混淆矩阵\n",
    "        cm = confusion_matrix(y_test_data, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # 详细指标计算\n",
    "        sensitivity = tp / (tp + fn)  # 敏感度/召回率\n",
    "        specificity = tn / (tn + fp)  # 特异度\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0  # 精确度\n",
    "        f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "        # PPV和NPV\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # 阳性预测值\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # 阴性预测值\n",
    "\n",
    "        # Matthews相关系数\n",
    "        mcc = matthews_corrcoef(y_test_data, y_pred)\n",
    "\n",
    "        # 阳性似然比和阴性似然比\n",
    "        lr_positive = sensitivity / (1 - specificity) if specificity != 1 else float('inf')\n",
    "        lr_negative = (1 - sensitivity) / specificity if specificity != 0 else float('inf')\n",
    "\n",
    "        results[disease] = {\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_acc,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'precision': precision,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'ppv': ppv,\n",
    "            'npv': npv,\n",
    "            'mcc': mcc,\n",
    "            'lr_positive': lr_positive,\n",
    "            'lr_negative': lr_negative,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "        print(f\"\\n=== {disease} 模型准确性检验 ===\")\n",
    "        print(f\"准确度 (Accuracy): {accuracy:.4f}\")\n",
    "        print(f\"平衡准确度 (Balanced Accuracy): {balanced_acc:.4f}\")\n",
    "        print(f\"敏感度 (Sensitivity/Recall): {sensitivity:.4f}\")\n",
    "        print(f\"特异度 (Specificity): {specificity:.4f}\")\n",
    "        print(f\"精确度 (Precision): {precision:.4f}\")\n",
    "        print(f\"F1分数: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        print(f\"阳性预测值 (PPV): {ppv:.4f}\")\n",
    "        print(f\"阴性预测值 (NPV): {npv:.4f}\")\n",
    "        print(f\"Matthews相关系数 (MCC): {mcc:.4f}\")\n",
    "        print(f\"阳性似然比 (LR+): {lr_positive:.4f}\")\n",
    "        print(f\"阴性似然比 (LR-): {lr_negative:.4f}\")\n",
    "        print(f\"混淆矩阵:\\n{cm}\")\n",
    "\n",
    "        # 绘制混淆矩阵热图\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Negative', 'Positive'],\n",
    "                    yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'{disease} Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig(f\"{disease.lower()}_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 执行评估\n",
    "evaluation_results = comprehensive_model_evaluation()"
   ],
   "id": "db22b20ae302969e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 交叉验证稳定性检验",
   "id": "e773cb2a1169346a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_validation_stability():\n",
    "    \"\"\"交叉验证稳定性检验\"\"\"\n",
    "\n",
    "    # 定义模型配置\n",
    "    model_configs = {\n",
    "        'Heart Disease': {\n",
    "            'model': LogisticRegression(random_state=42, solver='lbfgs', max_iter=2000),\n",
    "            'X_train': X_train_scaled,\n",
    "            'y_train': y_train\n",
    "        },\n",
    "        'Stroke': {\n",
    "            'model': LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', max_iter=1000),\n",
    "            'X_train': X_train_scaled,  # 使用stroke的训练数据\n",
    "            'y_train': y_train\n",
    "        },\n",
    "        'Cirrhosis': {\n",
    "            'model': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "            'X_train': X_train_scaled,  # 使用cirrhosis的训练数据\n",
    "            'y_train': y_train\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cv_results = {}\n",
    "\n",
    "    for disease, config in model_configs.items():\n",
    "        model_obj = config['model']\n",
    "        X_train_data = config['X_train']\n",
    "        y_train_data = config['y_train']\n",
    "\n",
    "        # 分层交叉验证\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "        # 多个评估指标的交叉验证\n",
    "        accuracy_scores = cross_val_score(model_obj, X_train_data, y_train_data, cv=skf, scoring='accuracy')\n",
    "        roc_auc_scores = cross_val_score(model_obj, X_train_data, y_train_data, cv=skf, scoring='roc_auc')\n",
    "        f1_scores = cross_val_score(model_obj, X_train_data, y_train_data, cv=skf, scoring='f1')\n",
    "        precision_scores = cross_val_score(model_obj, X_train_data, y_train_data, cv=skf, scoring='precision')\n",
    "        recall_scores = cross_val_score(model_obj, X_train_data, y_train_data, cv=skf, scoring='recall')\n",
    "\n",
    "        cv_results[disease] = {\n",
    "            'accuracy': accuracy_scores,\n",
    "            'roc_auc': roc_auc_scores,\n",
    "            'f1': f1_scores,\n",
    "            'precision': precision_scores,\n",
    "            'recall': recall_scores\n",
    "        }\n",
    "\n",
    "        print(f\"\\n=== {disease} 10折交叉验证结果 ===\")\n",
    "        print(f\"准确度: {accuracy_scores.mean():.4f} ± {accuracy_scores.std():.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_scores.mean():.4f} ± {roc_auc_scores.std():.4f}\")\n",
    "        print(f\"F1分数: {f1_scores.mean():.4f} ± {f1_scores.std():.4f}\")\n",
    "        print(f\"精确度: {precision_scores.mean():.4f} ± {precision_scores.std():.4f}\")\n",
    "        print(f\"召回率: {recall_scores.mean():.4f} ± {recall_scores.std():.4f}\")\n",
    "\n",
    "        # 稳定性检验（变异系数）\n",
    "        cv_accuracy = accuracy_scores.std() / accuracy_scores.mean()\n",
    "        cv_roc_auc = roc_auc_scores.std() / roc_auc_scores.mean()\n",
    "\n",
    "        print(f\"准确度变异系数: {cv_accuracy:.4f}\")\n",
    "        print(f\"ROC AUC变异系数: {cv_roc_auc:.4f}\")\n",
    "\n",
    "        if cv_accuracy < 0.1 and cv_roc_auc < 0.1:\n",
    "            print(\"✓ 模型稳定性良好\")\n",
    "        else:\n",
    "            print(\"⚠ 模型稳定性需要改进\")\n",
    "\n",
    "        # 可视化交叉验证结果\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics = ['Accuracy', 'ROC AUC', 'F1', 'Precision', 'Recall']\n",
    "        scores = [accuracy_scores, roc_auc_scores, f1_scores, precision_scores, recall_scores]\n",
    "\n",
    "        plt.boxplot(scores, tick_labels=metrics)\n",
    "        plt.title(f'{disease} Cross-Validation Results')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"{disease.lower()}_cv_results.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "# 执行稳定性检验\n",
    "cv_stability = cross_validation_stability()"
   ],
   "id": "8c20693114f3f068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 模型灵敏度分析",
   "id": "4b7d79becaa7e103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sensitivity_analysis():\n",
    "    \"\"\"模型灵敏度分析\"\"\"\n",
    "\n",
    "    def feature_perturbation_analysis(model, X_test, feature_names, perturbation_pct=0.1):\n",
    "        \"\"\"特征扰动分析\"\"\"\n",
    "        baseline_pred = model.predict_proba(X_test)[:, 1].mean()\n",
    "        sensitivity_results = {}\n",
    "\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            X_perturbed = X_test.copy()\n",
    "\n",
    "            if len(X_perturbed.shape) == 2:\n",
    "                original_values = X_perturbed[:, i].copy()\n",
    "\n",
    "                # 正向扰动\n",
    "                X_perturbed[:, i] = original_values * (1 + perturbation_pct)\n",
    "                pred_positive = model.predict_proba(X_perturbed)[:, 1].mean()\n",
    "\n",
    "                # 负向扰动\n",
    "                X_perturbed[:, i] = original_values * (1 - perturbation_pct)\n",
    "                pred_negative = model.predict_proba(X_perturbed)[:, 1].mean()\n",
    "\n",
    "                # 恢复原值\n",
    "                X_perturbed[:, i] = original_values\n",
    "            else:\n",
    "                # 处理DataFrame情况\n",
    "                original_values = X_perturbed[feature].copy()\n",
    "\n",
    "                X_perturbed[feature] = original_values * (1 + perturbation_pct)\n",
    "                pred_positive = model.predict_proba(X_perturbed)[:, 1].mean()\n",
    "\n",
    "                X_perturbed[feature] = original_values * (1 - perturbation_pct)\n",
    "                pred_negative = model.predict_proba(X_perturbed)[:, 1].mean()\n",
    "\n",
    "                X_perturbed[feature] = original_values\n",
    "\n",
    "            # 计算灵敏度\n",
    "            sensitivity_positive = (pred_positive - baseline_pred) / baseline_pred if baseline_pred != 0 else 0\n",
    "            sensitivity_negative = (pred_negative - baseline_pred) / baseline_pred if baseline_pred != 0 else 0\n",
    "\n",
    "            sensitivity_results[feature] = {\n",
    "                'sensitivity_positive': sensitivity_positive,\n",
    "                'sensitivity_negative': sensitivity_negative,\n",
    "                'average_sensitivity': (abs(sensitivity_positive) + abs(sensitivity_negative)) / 2\n",
    "            }\n",
    "\n",
    "        return sensitivity_results\n",
    "\n",
    "    # 对三种疾病模型进行灵敏度分析\n",
    "    models_for_sensitivity = {\n",
    "        'Heart Disease': (model, X_test_scaled, X.columns),\n",
    "        'Stroke': (model, X_test_scaled, X.columns),  # 使用对应的stroke模型\n",
    "        'Cirrhosis': (rf_model, X_test_scaled, X.columns)  # 使用对应的cirrhosis模型和数据\n",
    "    }\n",
    "\n",
    "    all_sensitivity_results = {}\n",
    "\n",
    "    for disease, (model_obj, X_test_data, feature_names) in models_for_sensitivity.items():\n",
    "        print(f\"\\n=== {disease} 灵敏度分析 ===\")\n",
    "\n",
    "        sensitivity_results = feature_perturbation_analysis(model_obj, X_test_data, feature_names)\n",
    "        all_sensitivity_results[disease] = sensitivity_results\n",
    "\n",
    "        # 排序并显示最敏感的特征\n",
    "        sorted_features = sorted(sensitivity_results.items(),\n",
    "                                 key=lambda x: x[1]['average_sensitivity'], reverse=True)\n",
    "\n",
    "        print(\"最敏感的前10个特征:\")\n",
    "        for feature, sens_data in sorted_features[:10]:\n",
    "            print(f\"{feature}: {sens_data['average_sensitivity']:.4f}\")\n",
    "\n",
    "        # 可视化敏感度\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        features = [item[0] for item in sorted_features[:15]]\n",
    "        sensitivities = [item[1]['average_sensitivity'] for item in sorted_features[:15]]\n",
    "\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(features)))\n",
    "        bars = plt.barh(range(len(features)), sensitivities, color=colors)\n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.title(f'{disease} 特征灵敏度分析')\n",
    "        plt.xlabel('平均灵敏度')\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        # 添加数值标签\n",
    "        for i, (bar, sensitivity) in enumerate(zip(bars, sensitivities)):\n",
    "            plt.text(sensitivity + max(sensitivities) * 0.01, bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{sensitivity:.3f}', va='center', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{disease.lower()}_sensitivity_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return all_sensitivity_results\n",
    "\n",
    "\n",
    "# 执行灵敏度分析\n",
    "sensitivity_results = sensitivity_analysis()"
   ],
   "id": "ed6660911d0537d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 模型改进策略",
   "id": "5d30d3832555a597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def model_improvement():\n",
    "    \"\"\"模型改进策略 - 针对所有三种疾病\"\"\"\n",
    "\n",
    "    print(\"=== 模型改进策略 ===\")\n",
    "\n",
    "    # 1. 超参数优化 - 针对所有三种疾病\n",
    "    def hyperparameter_optimization():\n",
    "\n",
    "        print(\"\\n--- 超参数优化 ---\")\n",
    "\n",
    "        optimized_models = {}\n",
    "\n",
    "        np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "        optimized_models = {}\n",
    "\n",
    "        # Heart disease optimization - fix the scaling and data issues\n",
    "        try:\n",
    "            df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "\n",
    "            # Check for any infinite or very large values\n",
    "            df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "            df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "\n",
    "            df_heart = pd.get_dummies(df_heart, columns=[\n",
    "                'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "\n",
    "            X_heart = df_heart.drop('HeartDisease', axis=1)\n",
    "            y_heart = df_heart['HeartDisease']\n",
    "\n",
    "            # Remove any remaining non-numeric columns and convert to float\n",
    "            X_heart = X_heart.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            # Check for and remove columns with zero variance\n",
    "            from sklearn.feature_selection import VarianceThreshold\n",
    "            variance_selector = VarianceThreshold(threshold=0.01)\n",
    "            X_heart = pd.DataFrame(variance_selector.fit_transform(X_heart),\n",
    "                                   columns=X_heart.columns[variance_selector.get_support()])\n",
    "\n",
    "            X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n",
    "                X_heart, y_heart, test_size=0.2, random_state=42, stratify=y_heart\n",
    "            )\n",
    "\n",
    "            # Use StandardScaler with clipping to prevent extreme values\n",
    "            scaler_heart = StandardScaler()\n",
    "            X_train_heart_scaled = scaler_heart.fit_transform(X_train_heart)\n",
    "\n",
    "            # Clip extreme values to prevent numerical issues\n",
    "            X_train_heart_scaled = np.clip(X_train_heart_scaled, -5, 5)\n",
    "\n",
    "            # Simplified parameter grid to avoid numerical issues\n",
    "            param_grid_heart = {\n",
    "                'C': [0.01, 0.1, 1.0],\n",
    "                'solver': ['liblinear'],  # More stable solver\n",
    "                'max_iter': [1000]\n",
    "            }\n",
    "\n",
    "            grid_search_heart = GridSearchCV(\n",
    "                LogisticRegression(random_state=42),\n",
    "                param_grid_heart, cv=3, scoring='roc_auc', n_jobs=1  # Reduce CV folds and disable parallel processing\n",
    "            )\n",
    "\n",
    "            grid_search_heart.fit(X_train_heart_scaled, y_train_heart)\n",
    "            optimized_models['heart_optimized'] = grid_search_heart.best_estimator_\n",
    "\n",
    "            print(\"心脏病逻辑回归模型优化:\")\n",
    "            print(f\"最佳参数: {grid_search_heart.best_params_}\")\n",
    "            print(f\"最佳CV分数: {grid_search_heart.best_score_:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"心脏病模型优化失败: {e}\")\n",
    "\n",
    "        # Similar fixes for stroke model\n",
    "        try:\n",
    "            df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "\n",
    "            # Clean the data\n",
    "            df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "            df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "\n",
    "            df_stroke = pd.get_dummies(df_stroke, columns=['gender', 'ever_married', 'work_type', 'Residence_type',\n",
    "                                                           'smoking_status'])\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "\n",
    "            X_stroke = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "            y_stroke = df_stroke['stroke_encoded']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_stroke = X_stroke.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            # Remove low variance features\n",
    "            variance_selector = VarianceThreshold(threshold=0.01)\n",
    "            X_stroke = pd.DataFrame(variance_selector.fit_transform(X_stroke),\n",
    "                                    columns=X_stroke.columns[variance_selector.get_support()])\n",
    "\n",
    "            X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
    "                X_stroke, y_stroke, test_size=0.2, random_state=42, stratify=y_stroke\n",
    "            )\n",
    "\n",
    "            scaler_stroke = StandardScaler()\n",
    "            X_train_stroke_scaled = scaler_stroke.fit_transform(X_train_stroke)\n",
    "            X_train_stroke_scaled = np.clip(X_train_stroke_scaled, -5, 5)\n",
    "\n",
    "            param_grid_stroke = {\n",
    "                'C': [0.01, 0.1, 1.0],\n",
    "                'class_weight': ['balanced'],\n",
    "                'solver': ['liblinear'],\n",
    "                'max_iter': [1000]\n",
    "            }\n",
    "\n",
    "            grid_search_stroke = GridSearchCV(\n",
    "                LogisticRegression(random_state=42),\n",
    "                param_grid_stroke, cv=3, scoring='roc_auc', n_jobs=1\n",
    "            )\n",
    "\n",
    "            grid_search_stroke.fit(X_train_stroke_scaled, y_train_stroke)\n",
    "            optimized_models['stroke_optimized'] = grid_search_stroke.best_estimator_\n",
    "\n",
    "            print(\"\\n中风逻辑回归模型优化:\")\n",
    "            print(f\"最佳参数: {grid_search_stroke.best_params_}\")\n",
    "            print(f\"最佳CV分数: {grid_search_stroke.best_score_:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"中风模型优化失败: {e}\")\n",
    "\n",
    "        # 肝硬化超参数优化\n",
    "        try:\n",
    "            df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "            status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "            df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "            df_cirrhosis = pd.get_dummies(df_cirrhosis,\n",
    "                                          columns=['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema',\n",
    "                                                   'Stage'], drop_first=True)\n",
    "            df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "            X_cirrhosis = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "            y_cirrhosis = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "            X_train_cirrhosis, X_test_cirrhosis, y_train_cirrhosis, y_test_cirrhosis = train_test_split(\n",
    "                X_cirrhosis, y_cirrhosis, test_size=0.2, random_state=42, stratify=y_cirrhosis\n",
    "            )\n",
    "\n",
    "            scaler_cirrhosis = StandardScaler()\n",
    "            X_train_cirrhosis_scaled = scaler_cirrhosis.fit_transform(X_train_cirrhosis)\n",
    "\n",
    "            param_grid_cirrhosis = {\n",
    "                'n_estimators': [50, 100],\n",
    "                'max_depth': [5, 10, None],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2],\n",
    "                'class_weight': ['balanced', None]\n",
    "            }\n",
    "\n",
    "            grid_search_cirrhosis = GridSearchCV(\n",
    "                RandomForestClassifier(random_state=42),\n",
    "                param_grid_cirrhosis, cv=5, scoring='roc_auc', n_jobs=-1\n",
    "            )\n",
    "\n",
    "            grid_search_cirrhosis.fit(X_train_cirrhosis_scaled, y_train_cirrhosis)\n",
    "            optimized_models['cirrhosis_optimized'] = grid_search_cirrhosis.best_estimator_\n",
    "\n",
    "            print(\"\\n肝硬化随机森林模型优化:\")\n",
    "            print(f\"最佳参数: {grid_search_cirrhosis.best_params_}\")\n",
    "            print(f\"最佳CV分数: {grid_search_cirrhosis.best_score_:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"肝硬化模型优化失败: {e}\")\n",
    "\n",
    "        return optimized_models\n",
    "\n",
    "    # 2. 集成学习改进 - 针对三种疾病\n",
    "    def ensemble_improvement():\n",
    "        print(\"\\n--- 集成学习改进 ---\")\n",
    "\n",
    "        ensemble_models = {}\n",
    "\n",
    "        # 心脏病集成学习\n",
    "        try:\n",
    "            df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "            df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_heart = pd.get_dummies(df_heart, columns=[\n",
    "                'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "\n",
    "            X_heart = df_heart.drop('HeartDisease', axis=1)\n",
    "            y_heart = df_heart['HeartDisease']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_heart = X_heart.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n",
    "                X_heart, y_heart, test_size=0.2, random_state=42, stratify=y_heart\n",
    "            )\n",
    "\n",
    "            scaler_heart = RobustScaler()\n",
    "            X_train_heart_scaled = scaler_heart.fit_transform(X_train_heart)\n",
    "            X_test_heart_scaled = scaler_heart.transform(X_test_heart)\n",
    "\n",
    "            # 创建集成模型\n",
    "            base_models = [\n",
    "                ('lr', LogisticRegression(C=1.0, random_state=42, max_iter=2000)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ]\n",
    "\n",
    "            ensemble_heart = VotingClassifier(estimators=base_models, voting='soft')\n",
    "            ensemble_heart.fit(X_train_heart_scaled, y_train_heart)\n",
    "\n",
    "            y_pred_ensemble = ensemble_heart.predict(X_test_heart_scaled)\n",
    "            y_pred_proba_ensemble = ensemble_heart.predict_proba(X_test_heart_scaled)[:, 1]\n",
    "\n",
    "            accuracy_ensemble = accuracy_score(y_test_heart, y_pred_ensemble)\n",
    "            roc_auc_ensemble = roc_auc_score(y_test_heart, y_pred_proba_ensemble)\n",
    "\n",
    "            ensemble_models['heart_ensemble'] = ensemble_heart\n",
    "            print(f\"心脏病集成模型 - 准确度: {accuracy_ensemble:.4f}, ROC AUC: {roc_auc_ensemble:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"心脏病集成学习失败: {e}\")\n",
    "\n",
    "        # 中风集成学习\n",
    "        try:\n",
    "            df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "            df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_stroke = pd.get_dummies(df_stroke, columns=[\n",
    "                'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "\n",
    "            X_stroke = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "            y_stroke = df_stroke['stroke_encoded']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_stroke = X_stroke.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
    "                X_stroke, y_stroke, test_size=0.2, random_state=42, stratify=y_stroke\n",
    "            )\n",
    "\n",
    "            scaler_stroke = StandardScaler()\n",
    "            X_train_stroke_scaled = scaler_stroke.fit_transform(X_train_stroke)\n",
    "            X_test_stroke_scaled = scaler_stroke.transform(X_test_stroke)\n",
    "\n",
    "            # 创建集成模型\n",
    "            base_models = [\n",
    "                ('lr', LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=2000)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ]\n",
    "\n",
    "            ensemble_stroke = VotingClassifier(estimators=base_models, voting='soft')\n",
    "            ensemble_stroke.fit(X_train_stroke_scaled, y_train_stroke)\n",
    "\n",
    "            y_pred_ensemble = ensemble_stroke.predict(X_test_stroke_scaled)\n",
    "            y_pred_proba_ensemble = ensemble_stroke.predict_proba(X_test_stroke_scaled)[:, 1]\n",
    "\n",
    "            accuracy_ensemble = accuracy_score(y_test_stroke, y_pred_ensemble)\n",
    "            roc_auc_ensemble = roc_auc_score(y_test_stroke, y_pred_proba_ensemble)\n",
    "\n",
    "            ensemble_models['stroke_ensemble'] = ensemble_stroke\n",
    "            print(f\"中风集成模型 - 准确度: {accuracy_ensemble:.4f}, ROC AUC: {roc_auc_ensemble:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"中风集成学习失败: {e}\")\n",
    "\n",
    "        # 肝硬化集成学习 (keep existing code as it's working)\n",
    "        try:\n",
    "            df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "            status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "            df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "            df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "                'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "            ], drop_first=True)\n",
    "            df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "            X_cirrhosis = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "            y_cirrhosis = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "            X_train_cirrhosis, X_test_cirrhosis, y_train_cirrhosis, y_test_cirrhosis = train_test_split(\n",
    "                X_cirrhosis, y_cirrhosis, test_size=0.2, random_state=42, stratify=y_cirrhosis\n",
    "            )\n",
    "\n",
    "            scaler_cirrhosis = StandardScaler()\n",
    "            X_train_cirrhosis_scaled = scaler_cirrhosis.fit_transform(X_train_cirrhosis)\n",
    "            X_test_cirrhosis_scaled = scaler_cirrhosis.transform(X_test_cirrhosis)\n",
    "\n",
    "            # 创建集成模型\n",
    "            base_models = [\n",
    "                ('lr', LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=2000)),\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "            ]\n",
    "\n",
    "            ensemble_cirrhosis = VotingClassifier(estimators=base_models, voting='soft')\n",
    "            ensemble_cirrhosis.fit(X_train_cirrhosis_scaled, y_train_cirrhosis)\n",
    "\n",
    "            y_pred_ensemble = ensemble_cirrhosis.predict(X_test_cirrhosis_scaled)\n",
    "            y_pred_proba_ensemble = ensemble_cirrhosis.predict_proba(X_test_cirrhosis_scaled)[:, 1]\n",
    "\n",
    "            accuracy_ensemble = accuracy_score(y_test_cirrhosis, y_pred_ensemble)\n",
    "            roc_auc_ensemble = roc_auc_score(y_test_cirrhosis, y_pred_proba_ensemble)\n",
    "\n",
    "            ensemble_models['cirrhosis_ensemble'] = ensemble_cirrhosis\n",
    "            print(f\"肝硬化集成模型 - 准确度: {accuracy_ensemble:.4f}, ROC AUC: {roc_auc_ensemble:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"肝硬化集成学习失败: {e}\")\n",
    "\n",
    "        return ensemble_models\n",
    "\n",
    "    # 3. 数据平衡技术改进 - 针对三种疾病\n",
    "    def data_balancing_improvement():\n",
    "        print(\"\\n--- 数据平衡技术改进 ---\")\n",
    "\n",
    "        balanced_models = {}\n",
    "\n",
    "        # 心脏病数据平衡\n",
    "        try:\n",
    "            df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "            df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_heart = pd.get_dummies(df_heart, columns=[\n",
    "                'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "\n",
    "            X_heart = df_heart.drop('HeartDisease', axis=1)\n",
    "            y_heart = df_heart['HeartDisease']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_heart = X_heart.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            print(f\"心脏病原始分布: {np.bincount(y_heart)}\")\n",
    "\n",
    "            # Apply SMOTE for balancing\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_heart_balanced, y_heart_balanced = smote.fit_resample(X_heart, y_heart)\n",
    "\n",
    "            print(f\"心脏病SMOTE后分布: {np.bincount(y_heart_balanced)}\")\n",
    "\n",
    "            X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n",
    "                X_heart_balanced, y_heart_balanced, test_size=0.2, random_state=42, stratify=y_heart_balanced\n",
    "            )\n",
    "\n",
    "            scaler_heart = StandardScaler()\n",
    "            X_train_heart_scaled = scaler_heart.fit_transform(X_train_heart)\n",
    "            X_test_heart_scaled = scaler_heart.transform(X_test_heart)\n",
    "\n",
    "            model_heart = LogisticRegression(C=1.0, random_state=42, max_iter=1000)\n",
    "            model_heart.fit(X_train_heart_scaled, y_train_heart)\n",
    "\n",
    "            y_pred = model_heart.predict(X_test_heart_scaled)\n",
    "            y_pred_proba = model_heart.predict_proba(X_test_heart_scaled)[:, 1]\n",
    "\n",
    "            accuracy = accuracy_score(y_test_heart, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test_heart, y_pred_proba)\n",
    "\n",
    "            balanced_models['heart_balanced'] = model_heart\n",
    "            print(f\"心脏病平衡数据模型 - 准确度: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"心脏病数据平衡失败: {e}\")\n",
    "\n",
    "        # 中风数据平衡\n",
    "        try:\n",
    "            df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "            df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_stroke = pd.get_dummies(df_stroke, columns=[\n",
    "                'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "\n",
    "            X_stroke = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "            y_stroke = df_stroke['stroke_encoded']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_stroke = X_stroke.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            print(f\"中风原始分布: {np.bincount(y_stroke)}\")\n",
    "\n",
    "            # Apply SMOTE for balancing\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_stroke_balanced, y_stroke_balanced = smote.fit_resample(X_stroke, y_stroke)\n",
    "\n",
    "            print(f\"中风SMOTE后分布: {np.bincount(y_stroke_balanced)}\")\n",
    "\n",
    "            X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
    "                X_stroke_balanced, y_stroke_balanced, test_size=0.2, random_state=42, stratify=y_stroke_balanced\n",
    "            )\n",
    "\n",
    "            scaler_stroke = StandardScaler()\n",
    "            X_train_stroke_scaled = scaler_stroke.fit_transform(X_train_stroke)\n",
    "            X_test_stroke_scaled = scaler_stroke.transform(X_test_stroke)\n",
    "\n",
    "            model_stroke = LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=1000)\n",
    "            model_stroke.fit(X_train_stroke_scaled, y_train_stroke)\n",
    "\n",
    "            y_pred = model_stroke.predict(X_test_stroke_scaled)\n",
    "            y_pred_proba = model_stroke.predict_proba(X_test_stroke_scaled)[:, 1]\n",
    "\n",
    "            accuracy = accuracy_score(y_test_stroke, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test_stroke, y_pred_proba)\n",
    "\n",
    "            balanced_models['stroke_balanced'] = model_stroke\n",
    "            print(f\"中风平衡数据模型 - 准确度: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"中风数据平衡失败: {e}\")\n",
    "\n",
    "        # 肝硬化数据平衡 (keep existing code as it's working)\n",
    "        try:\n",
    "            df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "            status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "            df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "            df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "                'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "            ], drop_first=True)\n",
    "            df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "            X_cirrhosis = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "            y_cirrhosis = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "            print(f\"肝硬化原始分布: {np.bincount(y_cirrhosis)}\")\n",
    "\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_cirrhosis_balanced, y_cirrhosis_balanced = smote.fit_resample(X_cirrhosis, y_cirrhosis)\n",
    "\n",
    "            print(f\"肝硬化SMOTE后分布: {np.bincount(y_cirrhosis_balanced)}\")\n",
    "\n",
    "            X_train_cirrhosis, X_test_cirrhosis, y_train_cirrhosis, y_test_cirrhosis = train_test_split(\n",
    "                X_cirrhosis_balanced, y_cirrhosis_balanced, test_size=0.2, random_state=42,\n",
    "                stratify=y_cirrhosis_balanced\n",
    "            )\n",
    "\n",
    "            scaler_cirrhosis = StandardScaler()\n",
    "            X_train_cirrhosis_scaled = scaler_cirrhosis.fit_transform(X_train_cirrhosis)\n",
    "            X_test_cirrhosis_scaled = scaler_cirrhosis.transform(X_test_cirrhosis)\n",
    "\n",
    "            model_cirrhosis = LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=1000)\n",
    "            model_cirrhosis.fit(X_train_cirrhosis_scaled, y_train_cirrhosis)\n",
    "\n",
    "            y_pred = model_cirrhosis.predict(X_test_cirrhosis_scaled)\n",
    "            y_pred_proba = model_cirrhosis.predict_proba(X_test_cirrhosis_scaled)[:, 1]\n",
    "\n",
    "            accuracy = accuracy_score(y_test_cirrhosis, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test_cirrhosis, y_pred_proba)\n",
    "\n",
    "            balanced_models['cirrhosis_balanced'] = model_cirrhosis\n",
    "            print(f\"肝硬化平衡数据模型 - 准确度: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"肝硬化数据平衡失败: {e}\")\n",
    "\n",
    "        return balanced_models\n",
    "\n",
    "    # 4. 阈值优化 - 针对三种疾病\n",
    "    def threshold_optimization():\n",
    "        print(\"\\n--- 阈值优化 ---\")\n",
    "\n",
    "        # Import f1_score locally to avoid naming conflicts\n",
    "        from sklearn.metrics import f1_score\n",
    "\n",
    "        threshold_models = {}\n",
    "\n",
    "        # 心脏病阈值优化\n",
    "        try:\n",
    "            df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "            df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_heart = pd.get_dummies(df_heart, columns=[\n",
    "                'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "\n",
    "            X_heart = df_heart.drop('HeartDisease', axis=1)\n",
    "            y_heart = df_heart['HeartDisease']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_heart = X_heart.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n",
    "                X_heart, y_heart, test_size=0.2, random_state=42, stratify=y_heart\n",
    "            )\n",
    "\n",
    "            scaler_heart = StandardScaler()\n",
    "            X_train_heart_scaled = scaler_heart.fit_transform(X_train_heart)\n",
    "            X_test_heart_scaled = scaler_heart.transform(X_test_heart)\n",
    "\n",
    "            # Train model\n",
    "            model_heart = LogisticRegression(C=1.0, random_state=42, max_iter=1000)\n",
    "            model_heart.fit(X_train_heart_scaled, y_train_heart)\n",
    "\n",
    "            # Get prediction probabilities\n",
    "            y_proba_heart = model_heart.predict_proba(X_test_heart_scaled)[:, 1]\n",
    "\n",
    "            # Optimize threshold based on F1 score\n",
    "            thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "            best_threshold = 0.5\n",
    "            best_f1 = 0\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_proba_heart >= threshold).astype(int)\n",
    "                f1 = f1_score(y_test_heart, y_pred_threshold)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "\n",
    "            # Apply best threshold\n",
    "            y_pred_optimized = (y_proba_heart >= best_threshold).astype(int)\n",
    "            accuracy_optimized = accuracy_score(y_test_heart, y_pred_optimized)\n",
    "\n",
    "            threshold_models['heart_threshold'] = {'model': model_heart, 'threshold': best_threshold}\n",
    "            print(f\"heart最佳阈值: {best_threshold:.4f}\")\n",
    "            print(f\"heart最佳F1分数: {best_f1:.4f}\")\n",
    "            print(f\"heart优化阈值后准确度: {accuracy_optimized:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"心脏病阈值优化失败: {e}\")\n",
    "\n",
    "        # 中风阈值优化\n",
    "        try:\n",
    "            df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "\n",
    "            # Clean the data first\n",
    "            df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "            df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "\n",
    "            # Properly encode categorical columns\n",
    "            df_stroke = pd.get_dummies(df_stroke, columns=[\n",
    "                'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "            ], drop_first=True)\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "\n",
    "            X_stroke = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "            y_stroke = df_stroke['stroke_encoded']\n",
    "\n",
    "            # Ensure all columns are numeric\n",
    "            X_stroke = X_stroke.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "            X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
    "                X_stroke, y_stroke, test_size=0.2, random_state=42, stratify=y_stroke\n",
    "            )\n",
    "\n",
    "            scaler_stroke = StandardScaler()\n",
    "            X_train_stroke_scaled = scaler_stroke.fit_transform(X_train_stroke)\n",
    "            X_test_stroke_scaled = scaler_stroke.transform(X_test_stroke)\n",
    "\n",
    "            # Train model\n",
    "            model_stroke = LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=1000)\n",
    "            model_stroke.fit(X_train_stroke_scaled, y_train_stroke)\n",
    "\n",
    "            # Get prediction probabilities\n",
    "            y_proba_stroke = model_stroke.predict_proba(X_test_stroke_scaled)[:, 1]\n",
    "\n",
    "            # Optimize threshold based on F1 score\n",
    "            thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "            best_threshold = 0.5\n",
    "            best_f1 = 0\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_proba_stroke >= threshold).astype(int)\n",
    "                f1 = f1_score(y_test_stroke, y_pred_threshold)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "\n",
    "            # Apply best threshold\n",
    "            y_pred_optimized = (y_proba_stroke >= best_threshold).astype(int)\n",
    "            accuracy_optimized = accuracy_score(y_test_stroke, y_pred_optimized)\n",
    "\n",
    "            threshold_models['stroke_threshold'] = {'model': model_stroke, 'threshold': best_threshold}\n",
    "            print(f\"stroke最佳阈值: {best_threshold:.4f}\")\n",
    "            print(f\"stroke最佳F1分数: {best_f1:.4f}\")\n",
    "            print(f\"stroke优化阈值后准确度: {accuracy_optimized:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"中风阈值优化失败: {e}\")\n",
    "\n",
    "        # 肝硬化阈值优化 (keep existing code as it's working)\n",
    "        try:\n",
    "            df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "            status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "            df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "            df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "                'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "            ], drop_first=True)\n",
    "            df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "            X_cirrhosis = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "            y_cirrhosis = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "            X_train_cirrhosis, X_test_cirrhosis, y_train_cirrhosis, y_test_cirrhosis = train_test_split(\n",
    "                X_cirrhosis, y_cirrhosis, test_size=0.2, random_state=42, stratify=y_cirrhosis\n",
    "            )\n",
    "\n",
    "            scaler_cirrhosis = StandardScaler()\n",
    "            X_train_cirrhosis_scaled = scaler_cirrhosis.fit_transform(X_train_cirrhosis)\n",
    "            X_test_cirrhosis_scaled = scaler_cirrhosis.transform(X_test_cirrhosis)\n",
    "\n",
    "            # Train model\n",
    "            model_cirrhosis = LogisticRegression(C=0.1, class_weight='balanced', random_state=42, max_iter=1000)\n",
    "            model_cirrhosis.fit(X_train_cirrhosis_scaled, y_train_cirrhosis)\n",
    "\n",
    "            # Get prediction probabilities\n",
    "            y_proba_cirrhosis = model_cirrhosis.predict_proba(X_test_cirrhosis_scaled)[:, 1]\n",
    "\n",
    "            # Optimize threshold based on F1 score\n",
    "            thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "            best_threshold = 0.5\n",
    "            best_f1 = 0\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                y_pred_threshold = (y_proba_cirrhosis >= threshold).astype(int)\n",
    "                f1 = f1_score(y_test_cirrhosis, y_pred_threshold)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "\n",
    "            # Apply best threshold\n",
    "            y_pred_optimized = (y_proba_cirrhosis >= best_threshold).astype(int)\n",
    "            accuracy_optimized = accuracy_score(y_test_cirrhosis, y_pred_optimized)\n",
    "\n",
    "            threshold_models['cirrhosis_threshold'] = {'model': model_cirrhosis, 'threshold': best_threshold}\n",
    "            print(f\"cirrhosis最佳阈值: {best_threshold:.4f}\")\n",
    "            print(f\"cirrhosis最佳F1分数: {best_f1:.4f}\")\n",
    "            print(f\"cirrhosis优化阈值后准确度: {accuracy_optimized:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"肝硬化阈值优化失败: {e}\")\n",
    "\n",
    "        return threshold_models\n",
    "\n",
    "    # 执行所有改进策略\n",
    "    all_improved_models = {}\n",
    "\n",
    "    try:\n",
    "        hyperparameter_models = hyperparameter_optimization()\n",
    "        all_improved_models.update(hyperparameter_models)\n",
    "    except Exception as e:\n",
    "        print(f\"超参数优化失败: {e}\")\n",
    "\n",
    "    try:\n",
    "        ensemble_models = ensemble_improvement()\n",
    "        all_improved_models.update(ensemble_models)\n",
    "    except Exception as e:\n",
    "        print(f\"集成学习失败: {e}\")\n",
    "\n",
    "    try:\n",
    "        balanced_models = data_balancing_improvement()\n",
    "        all_improved_models.update(balanced_models)\n",
    "    except Exception as e:\n",
    "        print(f\"数据平衡失败: {e}\")\n",
    "\n",
    "    try:\n",
    "        threshold_results = threshold_optimization()\n",
    "        all_improved_models['threshold_results'] = threshold_results\n",
    "    except Exception as e:\n",
    "        print(f\"阈值优化失败: {e}\")\n",
    "\n",
    "    return all_improved_models\n",
    "\n",
    "\n",
    "# 执行模型改进\n",
    "improved_models = model_improvement()"
   ],
   "id": "a40200ff52364d5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 改进效果对比",
   "id": "ec8677f4d2aa528a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def improvement_comparison():\n",
    "    \"\"\"改进效果对比\"\"\"\n",
    "\n",
    "    print(\"\\n=== 模型改进效果对比 ===\")\n",
    "\n",
    "    # Define a common evaluation dataset for each disease with consistent preprocessing\n",
    "    def prepare_evaluation_data():\n",
    "        \"\"\"准备一致的评估数据\"\"\"\n",
    "        evaluation_datasets = {}\n",
    "\n",
    "        # Heart Disease data preparation\n",
    "        df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "        df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "        df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "\n",
    "        df_heart = pd.get_dummies(df_heart, columns=[\n",
    "            'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "        ], drop_first=True)\n",
    "\n",
    "        le_heart = LabelEncoder()\n",
    "        df_heart['HeartDisease'] = le_heart.fit_transform(df_heart['HeartDisease'])\n",
    "\n",
    "        X_heart = df_heart.drop('HeartDisease', axis=1)\n",
    "        y_heart = df_heart['HeartDisease']\n",
    "        X_heart = X_heart.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "        X_train_heart, X_test_heart, y_train_heart, y_test_heart = train_test_split(\n",
    "            X_heart, y_heart, test_size=0.2, random_state=42, stratify=y_heart\n",
    "        )\n",
    "\n",
    "        scaler_heart = StandardScaler()\n",
    "        X_test_heart_scaled = scaler_heart.fit(X_train_heart).transform(X_test_heart)\n",
    "\n",
    "        evaluation_datasets['heart'] = (X_test_heart_scaled, y_test_heart)\n",
    "\n",
    "        # Stroke data preparation\n",
    "        df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "        df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "        df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "\n",
    "        df_stroke = pd.get_dummies(df_stroke, columns=[\n",
    "            'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "        ], drop_first=True)\n",
    "\n",
    "        le_stroke = LabelEncoder()\n",
    "        df_stroke['stroke_encoded'] = le_stroke.fit_transform(df_stroke['stroke'])\n",
    "\n",
    "        X_stroke = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1)\n",
    "        y_stroke = df_stroke['stroke_encoded']\n",
    "        X_stroke = X_stroke.select_dtypes(include=[np.number]).astype('float64')\n",
    "\n",
    "        X_train_stroke, X_test_stroke, y_train_stroke, y_test_stroke = train_test_split(\n",
    "            X_stroke, y_stroke, test_size=0.2, random_state=42, stratify=y_stroke\n",
    "        )\n",
    "\n",
    "        scaler_stroke = StandardScaler()\n",
    "        X_test_stroke_scaled = scaler_stroke.fit(X_train_stroke).transform(X_test_stroke)\n",
    "\n",
    "        evaluation_datasets['stroke'] = (X_test_stroke_scaled, y_test_stroke)\n",
    "\n",
    "        # Cirrhosis data preparation\n",
    "        df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "        status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "        df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "\n",
    "        df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "            'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "        ], drop_first=True)\n",
    "        df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "\n",
    "        X_cirrhosis = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "        y_cirrhosis = df_cirrhosis[\"Status_Encoded\"]\n",
    "\n",
    "        X_train_cirrhosis, X_test_cirrhosis, y_train_cirrhosis, y_test_cirrhosis = train_test_split(\n",
    "            X_cirrhosis, y_cirrhosis, test_size=0.2, random_state=42, stratify=y_cirrhosis\n",
    "        )\n",
    "\n",
    "        scaler_cirrhosis = StandardScaler()\n",
    "        X_test_cirrhosis_scaled = scaler_cirrhosis.fit(X_train_cirrhosis).transform(X_test_cirrhosis)\n",
    "\n",
    "        evaluation_datasets['cirrhosis'] = (X_test_cirrhosis_scaled, y_test_cirrhosis)\n",
    "\n",
    "        return evaluation_datasets\n",
    "\n",
    "    # Prepare consistent evaluation datasets\n",
    "    eval_datasets = prepare_evaluation_data()\n",
    "\n",
    "    # Original models performance (using consistent data)\n",
    "    original_models = {\n",
    "        'Heart Disease (LR)': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Stroke (LR)': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
    "        'Cirrhosis (RF)': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    }\n",
    "\n",
    "    # Train original models on consistent data and evaluate\n",
    "    for name, (disease_key, model) in zip(original_models.keys(),\n",
    "                                          [('heart', original_models['Heart Disease (LR)']),\n",
    "                                           ('stroke', original_models['Stroke (LR)']),\n",
    "                                           ('cirrhosis', original_models['Cirrhosis (RF)'])]):\n",
    "        X_test, y_test = eval_datasets[disease_key]\n",
    "\n",
    "        # For evaluation, we need to retrain on the same split data\n",
    "        # This is a simplified approach - you might want to save the trained models instead\n",
    "        try:\n",
    "            # Simple retraining for demonstration\n",
    "            if disease_key == 'heart':\n",
    "                df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "                df_heart = df_heart.replace([np.inf, -np.inf], np.nan)\n",
    "                df_heart = df_heart.fillna(df_heart.median(numeric_only=True))\n",
    "                df_heart = pd.get_dummies(df_heart, columns=[\n",
    "                    'Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS'\n",
    "                ], drop_first=True)\n",
    "                le = LabelEncoder()\n",
    "                df_heart['HeartDisease'] = le.fit_transform(df_heart['HeartDisease'])\n",
    "                X = df_heart.drop('HeartDisease', axis=1).select_dtypes(include=[np.number]).astype('float64')\n",
    "                y = df_heart['HeartDisease']\n",
    "                X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            elif disease_key == 'stroke':\n",
    "                df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "                df_stroke = df_stroke.replace([np.inf, -np.inf], np.nan)\n",
    "                df_stroke = df_stroke.fillna(df_stroke.median(numeric_only=True))\n",
    "                df_stroke = pd.get_dummies(df_stroke, columns=[\n",
    "                    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "                ], drop_first=True)\n",
    "                le = LabelEncoder()\n",
    "                df_stroke['stroke_encoded'] = le.fit_transform(df_stroke['stroke'])\n",
    "                X = df_stroke.drop(['stroke', 'stroke_encoded'], axis=1).select_dtypes(include=[np.number]).astype(\n",
    "                    'float64')\n",
    "                y = df_stroke['stroke_encoded']\n",
    "                X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            else:  # cirrhosis\n",
    "                df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "                status_mapping = {'D': 1, 'C': 0, 'CL': 0}\n",
    "                df_cirrhosis[\"Status_Encoded\"] = df_cirrhosis[\"Status\"].map(status_mapping)\n",
    "                df_cirrhosis = pd.get_dummies(df_cirrhosis, columns=[\n",
    "                    'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Stage'\n",
    "                ], drop_first=True)\n",
    "                df_cirrhosis = df_cirrhosis.drop([\"Status\"], axis=1)\n",
    "                X = df_cirrhosis.drop(\"Status_Encoded\", axis=1)\n",
    "                y = df_cirrhosis[\"Status_Encoded\"]\n",
    "                X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "            print(f\"原始{name}模型 - 准确度: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"原始{name}模型评估失败: {e}\")\n",
    "\n",
    "    # Evaluate improved models if they exist\n",
    "    if 'improved_models' in globals():\n",
    "        for model_type in ['optimized', 'ensemble', 'balanced', 'threshold']:\n",
    "            for disease in ['heart', 'stroke', 'cirrhosis']:\n",
    "                model_key = f\"{disease}_{model_type}\"\n",
    "                if model_key in improved_models:\n",
    "                    try:\n",
    "                        X_test, y_test = eval_datasets[disease]\n",
    "                        model_info = improved_models[model_key]\n",
    "\n",
    "                        if isinstance(model_info, dict) and 'model' in model_info:\n",
    "                            model = model_info['model']\n",
    "                        else:\n",
    "                            model = model_info\n",
    "\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "\n",
    "                        accuracy = accuracy_score(y_test, y_pred)\n",
    "                        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "                        print(f\"改进{model_key}模型 - 准确度: {accuracy:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"评估{model_key}模型失败: {e}\")\n",
    "\n",
    "    return eval_datasets\n",
    "\n",
    "\n",
    "# Execute the comparison\n",
    "comparison_results = improvement_comparison()"
   ],
   "id": "9f9ac0fd75812cc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 综合分析",
   "id": "c3203b684a40e43a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 设置中文字体显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 读取三种疾病的数据\n",
    "df_heart = pd.read_csv(\"cleaned_heart.csv\")\n",
    "df_stroke = pd.read_csv(\"cleaned_stroke.csv\")\n",
    "df_cirrhosis = pd.read_csv(\"cleaned_cirrhosis.csv\")\n",
    "\n",
    "print(\"=== 三种疾病数据基本信息 ===\")\n",
    "print(f\"心脏病数据: {df_heart.shape}\")\n",
    "print(f\"中风数据: {df_stroke.shape}\")\n",
    "print(f\"肝硬化数据: {df_cirrhosis.shape}\")"
   ],
   "id": "ed86adaaef9b77c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 提取共同特征进行分析\n",
    "def extract_common_features():\n",
    "    \"\"\"提取三种疾病的共同危险因素\"\"\"\n",
    "\n",
    "    # 心脏病关键特征\n",
    "    heart_features = ['age', 'sex', 'cholesterol', 'blood_pressure', 'diabetes']\n",
    "\n",
    "    # 中风关键特征\n",
    "    stroke_features = ['age', 'gender', 'hypertension', 'heart_disease', 'avg_glucose_level', 'smoking_status']\n",
    "\n",
    "    # 肝硬化关键特征\n",
    "    cirrhosis_features = ['Age', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "\n",
    "    # 共同危险因素分析\n",
    "    common_risk_factors = {\n",
    "        'age': ['老龄化', '所有三种疾病的主要危险因素'],\n",
    "        'gender/sex': ['性别差异', '男性在心脏病和肝硬化中风险更高'],\n",
    "        'hypertension': ['高血压', '心脏病和中风的共同危险因素'],\n",
    "        'diabetes': ['糖尿病', '增加心脏病和中风风险'],\n",
    "        'smoking': ['吸烟', '所有三种疾病的危险因素'],\n",
    "        'metabolic_syndrome': ['代谢综合征', '影响心脏、血管和肝脏健康']\n",
    "    }\n",
    "\n",
    "    return common_risk_factors\n",
    "\n",
    "\n",
    "common_factors = extract_common_features()\n",
    "print(\"\\n=== 三种疾病共同危险因素 ===\")\n",
    "for factor, description in common_factors.items():\n",
    "    print(f\"{factor}: {description[1]}\")"
   ],
   "id": "a8463f913931f3b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 构建综合疾病风险数据集\n",
    "def create_comprehensive_dataset():\n",
    "    \"\"\"创建包含三种疾病风险的综合数据集\"\"\"\n",
    "\n",
    "    # 模拟患者数据（基于真实数据分布）\n",
    "    np.random.seed(42)\n",
    "    n_patients = 5000\n",
    "\n",
    "    # 生成基础特征\n",
    "    data = {\n",
    "        'age': np.random.normal(55, 15, n_patients),\n",
    "        'sex': np.random.choice([0, 1], n_patients, p=[0.45, 0.55]),  # 0=female, 1=male\n",
    "        'bmi': np.random.normal(28, 5, n_patients),\n",
    "        'systolic_bp': np.random.normal(130, 20, n_patients),\n",
    "        'diastolic_bp': np.random.normal(85, 10, n_patients),\n",
    "        'cholesterol': np.random.normal(200, 40, n_patients),\n",
    "        'glucose': np.random.normal(100, 30, n_patients),\n",
    "        'smoking': np.random.choice([0, 1], n_patients, p=[0.7, 0.3]),\n",
    "        'alcohol': np.random.choice([0, 1, 2], n_patients, p=[0.3, 0.5, 0.2]),  # 0=none, 1=moderate, 2=heavy\n",
    "        'family_history': np.random.choice([0, 1], n_patients, p=[0.6, 0.4]),\n",
    "        'exercise': np.random.choice([0, 1], n_patients, p=[0.4, 0.6]),\n",
    "        'stress_level': np.random.uniform(1, 10, n_patients)\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 确保年龄在合理范围内\n",
    "    df['age'] = np.clip(df['age'], 18, 100)\n",
    "    df['bmi'] = np.clip(df['bmi'], 15, 50)\n",
    "    df['systolic_bp'] = np.clip(df['systolic_bp'], 90, 200)\n",
    "    df['diastolic_bp'] = np.clip(df['diastolic_bp'], 60, 120)\n",
    "    df['cholesterol'] = np.clip(df['cholesterol'], 120, 350)\n",
    "    df['glucose'] = np.clip(df['glucose'], 70, 300)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_comprehensive = create_comprehensive_dataset()\n",
    "print(\"\\n=== 综合数据集特征 ===\")\n",
    "print(df_comprehensive.describe())"
   ],
   "id": "e64679a9032d5a78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 基于医学知识定义疾病风险计算函数\n",
    "def calculate_disease_risks(df):\n",
    "    \"\"\"基于医学文献计算三种疾病的患病概率\"\"\"\n",
    "\n",
    "    # 心脏病风险计算（基于Framingham风险评分）\n",
    "    def heart_disease_risk(row):\n",
    "        risk = 0.0\n",
    "        # 年龄因子\n",
    "        if row['age'] > 65:\n",
    "            risk += 0.3\n",
    "        elif row['age'] > 55:\n",
    "            risk += 0.2\n",
    "        elif row['age'] > 45:\n",
    "            risk += 0.1\n",
    "\n",
    "        # 性别因子\n",
    "        if row['sex'] == 1: risk += 0.1  # 男性风险更高\n",
    "\n",
    "        # BMI因子\n",
    "        if row['bmi'] > 30:\n",
    "            risk += 0.15\n",
    "        elif row['bmi'] > 25:\n",
    "            risk += 0.08\n",
    "\n",
    "        # 血压因子\n",
    "        if row['systolic_bp'] > 140:\n",
    "            risk += 0.2\n",
    "        elif row['systolic_bp'] > 120:\n",
    "            risk += 0.1\n",
    "\n",
    "        # 胆固醇因子\n",
    "        if row['cholesterol'] > 240:\n",
    "            risk += 0.15\n",
    "        elif row['cholesterol'] > 200:\n",
    "            risk += 0.08\n",
    "\n",
    "        # 生活方式因子\n",
    "        if row['smoking'] == 1: risk += 0.2\n",
    "        if row['exercise'] == 0: risk += 0.1\n",
    "        if row['family_history'] == 1: risk += 0.15\n",
    "\n",
    "        return min(risk, 0.9)  # 最大风险90%\n",
    "\n",
    "    # 中风风险计算\n",
    "    def stroke_risk(row):\n",
    "        risk = 0.0\n",
    "        # 年龄因子（中风风险随年龄急剧增加）\n",
    "        if row['age'] > 75:\n",
    "            risk += 0.4\n",
    "        elif row['age'] > 65:\n",
    "            risk += 0.25\n",
    "        elif row['age'] > 55:\n",
    "            risk += 0.15\n",
    "        elif row['age'] > 45:\n",
    "            risk += 0.08\n",
    "\n",
    "        # 高血压是最大危险因素\n",
    "        if row['systolic_bp'] > 160:\n",
    "            risk += 0.3\n",
    "        elif row['systolic_bp'] > 140:\n",
    "            risk += 0.2\n",
    "        elif row['systolic_bp'] > 120:\n",
    "            risk += 0.1\n",
    "\n",
    "        # 糖尿病（用高血糖代表）\n",
    "        if row['glucose'] > 126:\n",
    "            risk += 0.2\n",
    "        elif row['glucose'] > 100:\n",
    "            risk += 0.1\n",
    "\n",
    "        # 其他因子\n",
    "        if row['smoking'] == 1: risk += 0.15\n",
    "        if row['sex'] == 1 and row['age'] > 45: risk += 0.05  # 男性中老年\n",
    "        if row['family_history'] == 1: risk += 0.1\n",
    "\n",
    "        return min(risk, 0.8)\n",
    "\n",
    "    # 肝硬化风险计算\n",
    "    def cirrhosis_risk(row):\n",
    "        risk = 0.0\n",
    "        # 年龄因子\n",
    "        if row['age'] > 60:\n",
    "            risk += 0.2\n",
    "        elif row['age'] > 50:\n",
    "            risk += 0.15\n",
    "        elif row['age'] > 40:\n",
    "            risk += 0.1\n",
    "\n",
    "        # 酗酒是主要危险因素\n",
    "        if row['alcohol'] == 2:\n",
    "            risk += 0.4  # 重度饮酒\n",
    "        elif row['alcohol'] == 1:\n",
    "            risk += 0.15  # 中度饮酒\n",
    "\n",
    "        # 肥胖和代谢因子\n",
    "        if row['bmi'] > 30: risk += 0.2\n",
    "        if row['glucose'] > 126: risk += 0.15  # 糖尿病\n",
    "\n",
    "        # 性别因子（男性肝硬化风险稍高）\n",
    "        if row['sex'] == 1: risk += 0.08\n",
    "\n",
    "        # 其他因子\n",
    "        if row['smoking'] == 1: risk += 0.1\n",
    "\n",
    "        return min(risk, 0.7)\n",
    "\n",
    "    df['heart_disease_risk'] = df.apply(heart_disease_risk, axis=1)\n",
    "    df['stroke_risk'] = df.apply(stroke_risk, axis=1)\n",
    "    df['cirrhosis_risk'] = df.apply(cirrhosis_risk, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_comprehensive = calculate_disease_risks(df_comprehensive)"
   ],
   "id": "fd604c75500a3f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 生成疾病标签（基于风险概率）\n",
    "def generate_disease_labels(df):\n",
    "    \"\"\"基于计算的风险概率生成疾病标签\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # 使用风险概率生成二进制标签\n",
    "    df['has_heart_disease'] = np.random.binomial(1, df['heart_disease_risk'])\n",
    "    df['has_stroke'] = np.random.binomial(1, df['stroke_risk'])\n",
    "    df['has_cirrhosis'] = np.random.binomial(1, df['cirrhosis_risk'])\n",
    "\n",
    "    # 计算共病情况\n",
    "    df['comorbidity_count'] = df['has_heart_disease'] + df['has_stroke'] + df['has_cirrhosis']\n",
    "\n",
    "    # 定义共病类别\n",
    "    df['heart_stroke'] = (df['has_heart_disease'] == 1) & (df['has_stroke'] == 1) & (df['has_cirrhosis'] == 0)\n",
    "    df['heart_cirrhosis'] = (df['has_heart_disease'] == 1) & (df['has_stroke'] == 0) & (df['has_cirrhosis'] == 1)\n",
    "    df['stroke_cirrhosis'] = (df['has_heart_disease'] == 0) & (df['has_stroke'] == 1) & (df['has_cirrhosis'] == 1)\n",
    "    df['all_three'] = (df['has_heart_disease'] == 1) & (df['has_stroke'] == 1) & (df['has_cirrhosis'] == 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_comprehensive = generate_disease_labels(df_comprehensive)\n",
    "\n",
    "# 年龄分组\n",
    "age_groups = pd.cut(df_comprehensive['age'], bins=[0, 30, 50, 65, 100],\n",
    "                    labels=['青年', '中年', '老年前期', '老年'])\n",
    "df_comprehensive['age_group'] = age_groups\n",
    "\n",
    "# 共病统计分析\n",
    "print(\"\\n=== 疾病患病率统计 ===\")\n",
    "print(f\"心脏病患病率: {df_comprehensive['has_heart_disease'].mean():.3f}\")\n",
    "print(f\"中风患病率: {df_comprehensive['has_stroke'].mean():.3f}\")\n",
    "print(f\"肝硬化患病率: {df_comprehensive['has_cirrhosis'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n=== 共病情况统计 ===\")\n",
    "print(f\"同时患心脏病和中风: {df_comprehensive['heart_stroke'].sum()} ({df_comprehensive['heart_stroke'].mean():.3f})\")\n",
    "print(\n",
    "    f\"同时患心脏病和肝硬化: {df_comprehensive['heart_cirrhosis'].sum()} ({df_comprehensive['heart_cirrhosis'].mean():.3f})\")\n",
    "print(\n",
    "    f\"同时患中风和肝硬化: {df_comprehensive['stroke_cirrhosis'].sum()} ({df_comprehensive['stroke_cirrhosis'].mean():.3f})\")\n",
    "print(f\"同时患三种疾病: {df_comprehensive['all_three'].sum()} ({df_comprehensive['all_three'].mean():.3f})\")\n",
    "\n",
    "# 共病分布可视化\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "comorbidity_counts = df_comprehensive['comorbidity_count'].value_counts().sort_index()\n",
    "plt.bar(comorbidity_counts.index, comorbidity_counts.values)\n",
    "plt.title('共病数量分布')\n",
    "plt.xlabel('同时患病数量')\n",
    "plt.ylabel('患者数量')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "disease_rates = [\n",
    "    df_comprehensive['has_heart_disease'].mean(),\n",
    "    df_comprehensive['has_stroke'].mean(),\n",
    "    df_comprehensive['has_cirrhosis'].mean()\n",
    "]\n",
    "plt.bar(['心脏病', '中风', '肝硬化'], disease_rates)\n",
    "plt.title('单一疾病患病率')\n",
    "plt.ylabel('患病率')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "comorbidity_rates = [\n",
    "    df_comprehensive['heart_stroke'].mean(),\n",
    "    df_comprehensive['heart_cirrhosis'].mean(),\n",
    "    df_comprehensive['stroke_cirrhosis'].mean(),\n",
    "    df_comprehensive['all_three'].mean()\n",
    "]\n",
    "plt.bar(['心脏病+中风', '心脏病+肝硬化', '中风+肝硬化', '三种疾病'], comorbidity_rates)\n",
    "plt.title('共病患病率')\n",
    "plt.ylabel('患病率')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 年龄分组的疾病分布\n",
    "plt.subplot(2, 3, 4)\n",
    "age_groups = pd.cut(df_comprehensive['age'], bins=[0, 40, 50, 60, 70, 100],\n",
    "                    labels=['<40', '40-50', '50-60', '60-70', '70+'])\n",
    "age_disease = df_comprehensive.groupby(age_groups, observed=False)[\n",
    "    ['has_heart_disease', 'has_stroke', 'has_cirrhosis']].mean()\n",
    "age_disease.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('不同年龄组疾病患病率')\n",
    "plt.ylabel('患病率')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 性别差异分析\n",
    "plt.subplot(2, 3, 5)\n",
    "gender_disease = df_comprehensive.groupby('sex')[['has_heart_disease', 'has_stroke', 'has_cirrhosis']].mean()\n",
    "gender_disease.index = ['女性', '男性']\n",
    "gender_disease.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('性别疾病患病率差异')\n",
    "plt.ylabel('患病率')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# 风险因子相关性热图\n",
    "plt.subplot(2, 3, 6)\n",
    "risk_factors = ['age', 'bmi', 'systolic_bp', 'cholesterol', 'glucose']\n",
    "disease_outcomes = ['has_heart_disease', 'has_stroke', 'has_cirrhosis']\n",
    "correlation_matrix = df_comprehensive[risk_factors + disease_outcomes].corr()\n",
    "sns.heatmap(correlation_matrix.loc[risk_factors, disease_outcomes], annot=True, cmap='RdBu_r', center=0)\n",
    "plt.title('危险因子与疾病相关性')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comprehensive_disease_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "ed25c07619bc9b6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 构建共病预测模型\n",
    "def build_comorbidity_models(df):\n",
    "    \"\"\"构建预测共病的机器学习模型\"\"\"\n",
    "\n",
    "    # 特征选择\n",
    "    feature_cols = ['age', 'sex', 'bmi', 'systolic_bp', 'diastolic_bp', 'cholesterol',\n",
    "                    'glucose', 'smoking', 'alcohol', 'family_history', 'exercise', 'stress_level']\n",
    "\n",
    "    X = df[feature_cols]\n",
    "\n",
    "    # 特征缩放\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "    # 定义多个预测目标\n",
    "    targets = {\n",
    "        'heart_stroke': df['heart_stroke'].astype(int),\n",
    "        'heart_cirrhosis': df['heart_cirrhosis'].astype(int),\n",
    "        'stroke_cirrhosis': df['stroke_cirrhosis'].astype(int),\n",
    "        'all_three': df['all_three'].astype(int)\n",
    "    }\n",
    "\n",
    "    models = {}\n",
    "    results = {}\n",
    "\n",
    "    for target_name, y in targets.items():\n",
    "        print(f\"\\n=== {target_name} 预测模型 ===\")\n",
    "\n",
    "        # 划分训练测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # 随机森林模型\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            max_depth=10\n",
    "        )\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 逻辑回归模型\n",
    "        lr_model = LogisticRegression(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000\n",
    "        )\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        lr_pred = lr_model.predict(X_test)\n",
    "        lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # 模型评估\n",
    "        rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "        lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "        try:\n",
    "            rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "            lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "        except ValueError:\n",
    "            rf_auc = lr_auc = 0.5  # 当只有一个类别时\n",
    "\n",
    "        print(f\"随机森林 - Accuracy: {rf_accuracy:.4f}, AUC: {rf_auc:.4f}\")\n",
    "        print(f\"逻辑回归 - Accuracy: {lr_accuracy:.4f}, AUC: {lr_auc:.4f}\")\n",
    "\n",
    "        # 特征重要性分析\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_cols,\n",
    "            'RF_Importance': rf_model.feature_importances_,\n",
    "            'LR_Coefficient': np.abs(lr_model.coef_[0])\n",
    "        }).sort_values(by='RF_Importance', ascending=False)\n",
    "\n",
    "        print(f\"Top 5 重要特征:\")\n",
    "        print(feature_importance.head())\n",
    "\n",
    "        models[target_name] = {\n",
    "            'rf_model': rf_model,\n",
    "            'lr_model': lr_model,\n",
    "            'scaler': scaler,\n",
    "            'feature_cols': feature_cols\n",
    "        }\n",
    "\n",
    "        results[target_name] = {\n",
    "            'rf_accuracy': rf_accuracy,\n",
    "            'rf_auc': rf_auc,\n",
    "            'lr_accuracy': lr_accuracy,\n",
    "            'lr_auc': lr_auc,\n",
    "            'feature_importance': feature_importance\n",
    "        }\n",
    "\n",
    "    return models, results\n",
    "\n",
    "\n",
    "models, results = build_comorbidity_models(df_comprehensive)"
   ],
   "id": "7b52b26671490549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 综合风险评估函数\n",
    "def comprehensive_risk_assessment(patient_data, models):\n",
    "    \"\"\"为单个患者计算所有疾病和共病风险\"\"\"\n",
    "\n",
    "    # 确保输入数据格式正确\n",
    "    if isinstance(patient_data, dict):\n",
    "        patient_df = pd.DataFrame([patient_data])\n",
    "    else:\n",
    "        patient_df = patient_data.copy()\n",
    "\n",
    "    # 特征缩放\n",
    "    feature_cols = models['heart_stroke']['feature_cols']\n",
    "    scaler = models['heart_stroke']['scaler']\n",
    "    patient_scaled = scaler.transform(patient_df[feature_cols])\n",
    "    patient_scaled = pd.DataFrame(patient_scaled, columns=feature_cols)\n",
    "\n",
    "    # 预测各种共病概率\n",
    "    risk_assessment = {}\n",
    "\n",
    "    for condition, model_dict in models.items():\n",
    "        rf_model = model_dict['rf_model']\n",
    "        lr_model = model_dict['lr_model']\n",
    "\n",
    "        rf_prob = rf_model.predict_proba(patient_scaled)[:, 1][0]\n",
    "        lr_prob = lr_model.predict_proba(patient_scaled)[:, 1][0]\n",
    "\n",
    "        # 使用两个模型的平均值作为最终预测\n",
    "        avg_prob = (rf_prob + lr_prob) / 2\n",
    "\n",
    "        risk_assessment[condition] = {\n",
    "            'rf_probability': rf_prob,\n",
    "            'lr_probability': lr_prob,\n",
    "            'average_probability': avg_prob,\n",
    "            'risk_level': 'High' if avg_prob > 0.3 else 'Medium' if avg_prob > 0.1 else 'Low'\n",
    "        }\n",
    "\n",
    "    return risk_assessment\n",
    "\n",
    "\n",
    "# 示例患者风险评估\n",
    "example_patients = [\n",
    "    {\n",
    "        'age': 65, 'sex': 1, 'bmi': 32, 'systolic_bp': 150, 'diastolic_bp': 95,\n",
    "        'cholesterol': 240, 'glucose': 130, 'smoking': 1, 'alcohol': 2,\n",
    "        'family_history': 1, 'exercise': 0, 'stress_level': 8\n",
    "    },\n",
    "    {\n",
    "        'age': 45, 'sex': 0, 'bmi': 24, 'systolic_bp': 120, 'diastolic_bp': 80,\n",
    "        'cholesterol': 180, 'glucose': 90, 'smoking': 0, 'alcohol': 0,\n",
    "        'family_history': 0, 'exercise': 1, 'stress_level': 4\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n=== 患者风险评估示例 ===\")\n",
    "for i, patient in enumerate(example_patients, 1):\n",
    "    print(f\"\\n患者 {i} 特征:\")\n",
    "    for key, value in patient.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    assessment = comprehensive_risk_assessment(patient, models)\n",
    "    print(f\"\\n患者 {i} 共病风险评估:\")\n",
    "    for condition, risk in assessment.items():\n",
    "        print(f\"  {condition}: {risk['average_probability']:.3f} ({risk['risk_level']})\")"
   ],
   "id": "82e22bd1063109c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 疾病风险可视化仪表板\n",
    "def create_risk_dashboard(assessment_results):\n",
    "    \"\"\"创建风险评估可视化仪表板\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    conditions = ['heart_stroke', 'heart_cirrhosis', 'stroke_cirrhosis', 'all_three']\n",
    "    condition_labels = ['心脏病+中风', '心脏病+肝硬化', '中风+肝硬化', '三种疾病']\n",
    "\n",
    "    for i, (patient_id, assessment) in enumerate(assessment_results.items()):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "\n",
    "        probabilities = [assessment[condition]['average_probability'] for condition in conditions]\n",
    "        colors = ['red' if p > 0.3 else 'orange' if p > 0.1 else 'green' for p in probabilities]\n",
    "\n",
    "        bars = plt.bar(condition_labels, probabilities, color=colors)\n",
    "        plt.title(f'{patient_id} 共病风险评估')\n",
    "        plt.ylabel('风险概率')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # 添加数值标签\n",
    "        for bar, prob in zip(bars, probabilities):\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "                     f'{prob:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        # 添加风险等级线\n",
    "        plt.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='高风险')\n",
    "        plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='中风险')\n",
    "\n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"comprehensive_risk_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 为示例患者创建风险仪表板\n",
    "assessment_results = {}\n",
    "for i, patient in enumerate(example_patients, 1):\n",
    "    assessment_results[f'患者{i}'] = comprehensive_risk_assessment(patient, models)\n",
    "\n",
    "create_risk_dashboard(assessment_results)"
   ],
   "id": "6ce6d1cd02a9bf4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 完结，撒花！",
   "id": "904b829f134c7f71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
